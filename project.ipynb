{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: **Name of method** in **name of software**\n",
    "\n",
    "## Group members\n",
    "* Darrien Lee\n",
    "* Peter Minwegen\n",
    "* Nicholas Palmer\n",
    "* Benji Goldstein\n",
    "\n",
    "This notebook is a blank slate for you to write in.  Feel free to include figures (don't forget to add/commit them to your repository) and examples.  You can change the kernel (from `Python 3`; see upper right) if the open source project you're writing about does not use Python.  You can write from the prompts below or delete all the cells and start fresh.  Note that Git will always contain your history.\n",
    "\n",
    "You can run shell commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numdifftools as nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project.ipynb  README.md\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and include code snippets\n",
    "\n",
    "```c\n",
    "double square(double x) {\n",
    "    return x*x;\n",
    "}\n",
    "```\n",
    "or code cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "square(3) = 9\n"
     ]
    }
   ],
   "source": [
    "def square(x):\n",
    "    return x*x\n",
    "\n",
    "print(f'square(3) = {square(3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some of the content for this section was used from Nicholas Palmer's individual project on Richardson Extrapolation, and that all of the content of this notebook is solely the work of the group members listed above.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Describe the objective of your study, citing prior work as appropriate (papers, websites, etc.).  There is no requirement on citation style, but please try to be consistent.\n",
    "\n",
    "The objective of the study for this project is to \"break\" the numdiftools software by violating some assumptions on the inputs for the software, as well as attempting to violate the assumptions of the Richardson Extrapolation method. In this section we will describe the Richardson Extrapolation method and how it is useful for numerical differentiation. Then in the following sections we will describe the methodology for performing these tests and studies, how that affects usability and usefulness of numdiftools for the average user, any conclusions as a result of the study, and finally any open questions at the end of the study.  \n",
    "\n",
    "Richardson extrapolation can be used to obtain high numerical accuracy by taking the linear combination of successive approximations of numerical derivatives. This method is described below: \n",
    "\n",
    "We can calculate the derivative of a function $f(x)$ by using the centered limit definition of the derivative:\n",
    "\n",
    "$$f'(x) = \\lim_{h \\rightarrow 0} \\frac{f(x + h) - f(x - h)}{2h}$$\n",
    "\n",
    "Where $h$ is the step size. This, of course, assumes that we can make $h$ arbitrarily small. However, numerically this is impossible, as computers have a limit on the range of numbers that they can represent. Therefore, using this method, we can only represent values of $h$ that are as small as $\\epsilon_{machine}$. Therefore, since we must use representable step sizes for $h$, we can rework the above definition to include an error term: \n",
    "\n",
    "$$f'(x) = \\frac{f(x + h) - f(x - h)}{2h} + O(h^2)$$\n",
    "\n",
    "From this we can see that the error term is on the order of $h^2$. From this, it must seem that when we reduce $h$ by half, we should reduce the error by a factor of 4. Let us see how this works in practice below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Note that this code was taken from the 4/7 in class activity for illustrative purposes\n",
    "#Slight modifications were made\n",
    "def compute_errors(diff, f, fp, x=np.linspace(-4, 4)):\n",
    "    hs = np.geomspace(1e-15, 1, 16)\n",
    "    errors = [np.linalg.norm(diff(f, x, h) - fp(x), np.inf) for h in hs]\n",
    "    return hs, errors\n",
    "\n",
    "\n",
    "def diff2(f, x, h):\n",
    "    return (f(x + h) - f(x - h)) / (2*h)\n",
    "\n",
    "hs, errors = compute_errors(diff2, np.sin, np.cos)\n",
    "\n",
    "plt.loglog(hs, errors, 'o', label='num diff')\n",
    "plt.xlabel('$h$')\n",
    "plt.ylabel('max error')\n",
    "plt.ylim(1e-12, 1)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main reason that we see this type of behavior is due to subtractive cancellation. Subtractive cancellation is the error that is introduced by subtracting two values with finite accuracy that are very close to each other. Recall the original function for computing the derivative of a function:\n",
    "\n",
    "$$f'(x) = \\frac{f(x + h) - f(x - h)}{2h} + O(h^2)$$\n",
    "\n",
    "As $h$ gets smaller and smaller, the values of $f(x + h)$ and $f(x - h)$ successively get closer together. Because the calculation for $f(x)$ has finite accuracy, we begin to require greater and greater accuracy in our calculation of $f(x)$ to avoid this subtractive cancellation error. We can see this in action below. \n",
    "\n",
    "Suppose we want to calculate $f(x + h) - f(x)$ with 10 digits of accuracy. If $h = 0.00001$, $x = 1$, and $f(x) = sin(x)$, we need 15 digits of accuracy when calculating $sin(1.01)$, since the first 5 digits of the calculation cancel to zero. \n",
    "\n",
    "$$\\frac{sin(1.00001) - sin(1)}{0.1}$$\n",
    "\n",
    "Doing the calculations we get the following: \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&0.841476387788881\\\\   \n",
    "-\\phantom{y=}&&\\\\    \n",
    "&0.841470984807896\\\\\\hline   \n",
    "&0.000005402980985    \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So, from this we can see that when $h$ get sufficiently small, we begin to get subtractive cancellation due to the finite accuracy of our calculation of $f(x)$.\n",
    "\n",
    "<hr>\n",
    "\n",
    "We can again rework the above definition to see that the error is proportional to $h^2$.\n",
    "\n",
    "$$f'(x) \\approx \\frac{f(x + h) - f(x - h)}{2h} + Ch^2$$\n",
    "\n",
    "Now we can see that the error in the above term is proportional to some constant C. For the sake of syntactical simplicity, let us define the centered differentiation of a function $f$ at a point $x$ with step size $h$ as follows: \n",
    "\n",
    "$$D(f, x, h) = \\frac{f(x + h) - f(x - h)}{2h}$$\n",
    "\n",
    "Where the error term can be expressed as $Ch^2$. We now know that the true value for a derivative $T_v$ can be approximated as follows: \n",
    "\n",
    "$$T_v = D(f, x, h) + Ch^2$$\n",
    "\n",
    "When we halve the step size to $h/2$, we obtain the following: \n",
    "\n",
    "$$T_v = D(f, x, \\frac{h}{2}) + C\\frac{h^2}{4}$$\n",
    "\n",
    "Now, we can solve the above system of equationms for $T_v$ by subtracting out the term $C$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&T_v = D(f, x, h) + Ch^2\\\\   \n",
    "-\\phantom{y=}&&\\\\    \n",
    "&4T_v = 4D(f, x, \\frac{h}{2}) + Ch^2\\\\\\hline   \n",
    "&-3T_v = D(f, x, h) - 4D(f, x, \\frac{h}{2})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Simplifying: \n",
    "\n",
    "$$T_v = \\frac{4D(f, x, \\frac{h}{2}) - D(f, x, h)}{3}$$\n",
    "\n",
    "Now, we have again solved for the value $T_v$, however, we now avoid the subtractive cancellation error, as $4D(f, x, \\frac{h}{2})$ and $F(f, x, h)$ are not so close to each other any more.\n",
    "\n",
    "In fact, we see that the formula for each entry is as follows: \n",
    "\n",
    "$$R_{i, n} = \\frac{2^n R_{i, n-1} - R_{i-1, n-1}}{2^n - 1}$$\n",
    "\n",
    "If we think of these entries as being a matrix, we notice that each entry is simply a linear combination of two other entries. Using this we can write a function which allows us to calculate the first column of entries, and then successively calculate the next columns in the matrix.\n",
    "\n",
    "The only limit on how many times this can be repeated is how small $h$ can be represented on the machine. Below we calculate how many times this could be repeated (the results are as calculated on my machine). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1008551026.2393295,\n",
       " 5259.420067530658,\n",
       " -109.13363150838231,\n",
       " -4.346399023253627,\n",
       " -0.28230174561749743]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#12th order derivative of sin(1/x)\n",
    "def highdev(x):\n",
    "    return (\n",
    "        (np.sin(1/x)/x**24)-(132*np.cos(1/x)/x**23)-(7260*np.sin(1/x)/x**22)+(217800*np.cos(1/x)/x**21)+(3920400*np.sin(1/x)/x**20)\n",
    "        -(43908480*np.cos(1/x)/x**19)-(307359360*np.sin(1/x)/x**18)+(1317254500*np.cos(1/x)/x**17)+(3293136000*np.sin(1/x)/x**16)\n",
    "        -(439084800*np.cos(1/x)/x**15)-(2634508800*np.sin(1/x)/x**14)+(479001600*np.cos(1/x)/x**13)\n",
    "        )\n",
    "highdev(1)\n",
    "def f(x):\n",
    "    return np.sin(1/x)\n",
    "df = nd.Derivative(f,12)\n",
    "error=[0,0,0,0,0]\n",
    "for x in range(5):\n",
    "    error[x]=df(x+1)-highdev(x+1)\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and interpretation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions and open questions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
